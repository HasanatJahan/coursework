---
title: "ISLR Lab 5"
output: html_notebook
---
## Labs 5.3.1, 5.3.2, 5.3.3

### Lab 5.3.1: Validation Set Approach

```{r}
library(ISLR)
library(tidyverse)
library(dplyr)
set.seed(69)
colnames(Auto)
#attach(Auto)
train <- sample(392, 196)
#train
```
Now we take that subset of data and fit a linear regression model 
```{r}
lm.fit.auto <- lm(mpg~horsepower, data = Auto, subset = train)
par(mfrow=c(2,2))
plot(lm.fit)
summary(lm.fit)
```

We now use the predict() function to estimate the response for all 392
observations, and we use the mean() function to calculate the MSE of the
196 observations in the validation set. Note that the -train index below
selects only the observations that are not in the training set.

```{r}
attach(Auto)
mean((mpg-predict(lm.fit, Auto))[-train]^2)
```

We can use poly to estimate the test error for quadratic and cubic regressions
```{r}
lm.fit2 <- lm(mpg~poly(horsepower, 2), data = Auto, subset = train)
mean((mpg-predict(lm.fit2, Auto))[-train]^2)


```
```{r}
lm.fit3 <- lm(mpg~poly(horsepower, 3), data=Auto, subset=train)
mean((mpg-predict(lm.fit3, Auto))[-train]^2)
```

### Lab 5.3.2: Leave One Out Cross Validation 

The glm() function to perform logistic regression by passing
in the family="binomial" argument. But if we use glm() to fit a model
without passing in the family argument, then it performs linear regression,
just like the lm() function

```{r}
glm.fit <- glm(mpg~horsepower, data=Auto)
coef(glm.fit)
par(mfrow=c(2,2))
summary(glm.fit)
plot(glm.fit)

```
Let's check if they are both the same 
```{r}
lm.fit <- lm(mpg~horsepower, data=Auto)
summary(lm.fit)

```
They are the same! 
The glm() function can be used with cv.glm()
The two numbers in the delta vector contain the cross-validation results. 
```{r}
library(boot)
glm.fit <- glm(mpg~horsepower, data=Auto)

cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
```

We can automate the process for increasingly complex polynomials 
```{r}
cv.error <- rep(0,5)
for(i in 1:5)
{
  glm.fit <- glm(mpg~poly(horsepower, i), data=Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
```



### Lab 5.3.3: K-fold cross validation 

```{r}
set.seed(42)
cv.error.10 = rep(0,10)
for(i in 1:10)
{
  glm.fit <- glm(mpg~poly(horsepower, i), data=Boston)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K=10)$delta[1]
}
cv.error.10
```






















