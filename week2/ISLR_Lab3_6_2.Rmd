---
title: "LAB 3.6.2 ISLR"
output: html_notebook
---
Import the libraries

```{r}
library(MASS)
library(ISLR)
```

```{r}
#fix(Boston)
names(Boston)
?Boston

```

```{r}
attach(Boston)
lm.fit=lm(medv~lstat)
summary(lm.fit)
names(lm.fit)
```
To extract coefficients 
```{r}
coef(lm.fit)
```
```{r}
# the confidence interval 
confint(lm.fit)

# produce confidence intervals 
predict (lm.fit ,data.frame(lstat=c(5,10 ,15)),interval ="confidence")

# produce prediction intervals
predict(lm.fit, data.frame(lstat=c(5, 10, 15)), interval="prediction")



```
Now let's get plotting!
```{r}
# lwd is for line thickness
plot(lstat, medv)
abline(lm.fit)
abline (lm.fit ,lwd =3)
abline (lm.fit ,lwd=3 ,col ="red")


```

```{r}
plot(lstat ,medv,lwd = 5 ,col="blue")
plot(lstat ,medv ,pch =20)
plot(1:20,1:20,pch =1:20)


```

This plot is too cool for school
```{r}
plot(lstat ,medv ,pch ="ðŸ˜Ž")

```

par() is used to divide the plotting region
```{r}
par(mfrow=c(2,2))
plot(lm.fit)

```

Residuals from linear regression fit and rstident()
```{r}
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
```


Leverage statistics can be computed for any number of predictors using the hatvalues() function

```{r}
plot(hatvalues(lm.fit))
# now to tell us the largest leverage statistic 
which.max(hatvalues(lm.fit))
```



##############################################################


## Lab 3.6.3 throught 3.6.6

### Lab 3.6.3 
```{r}
lm.fit = lm(medv~lstat+age, data=Boston)
summary(lm.fit)

summary(lm.fit)$sigma

```
```{r}
# install.packages("car")
# library(car)
# vif(lm.fit)
```

If want to do regression with everything except one variable 
```{r}
# this is the model 
lm.fit1 = lm(medv~.-age, data = Boston)
summary(lm.fit1)
```
This is an alternative 
```{r}

lm.fit1 = update(lm.fit, ~.-age)
lm.fit1

```

### Lab 3.6.4
The syntax lstat:black tells R to inlude an interaction term between lstat and black. 
lstat*age is shorthand for lstat+age+lstat:age
```{r}
summary(lm(medv~lstat*age, data=Boston))
```

### Lab 3.6.5

```{r}
lm.fit2 = lm(medv~(lstat+I(lstat^2)))
summary(lm.fit2)
```
The near-zero p-value associated with the quadratic term suggests that
it leads to an improved model. We use the anova() function to further anova() quantify the extent to which the quadratic fit is superior to the linear fit

```{r}
lmfit <- lm(medv~lstat)
anova(lm.fit, lm.fit1)
```


```{r}
par(mfrow=c(2,2)) #multiple graphs in one
plot(lm.fit2)
```
For polynomials we can use the poly function 

```{r}
lm.fit5=lm(medv~poly(lstat,5))
summary(lm.fit5)
summary(lm(medv~log(rm), data=Boston))
```

## Exercise 3.6.6

```{r}
#fix(Carseats)
names(Carseats)
```


```{r}
lm.fit <- lm(Sales~.+Income:Advertising+Price:Age, data=Carseats)
summary(lm.fit)
plot(lm.fit)
```
The contrasts() function returns the coding that R uses for the dummy contrasts() variables.
```{r}
attach(Carseats)
contrasts(ShelveLoc)
```
The fact that the coefficient for
ShelveLocGood in the regression output is positive indicates that a good
shelving location is associated with high sales (relative to a bad location).
And ShelveLocMedium has a smaller positive coefficient, indicating that a
medium shelving location leads to higher sales than a bad shelving location
but lower sales than a good shelving location.




## Labs 5.3.1, 5.3.2, 5.3.3

### Lab 5.3.1

```{r}
library(ISLR)
library(tidyverse)
library(dplyr)
set.seed(42)
colnames(Auto)
attach(Auto)
train <- sample(392, 196)
#train
```
Now we take that subset of data and fit a linear regression model 
```{r}
lm.fit.auto <- lm(mpg~horsepower, data = Auto, subset = train)
par(mfrow=c(2,2))
plot(lm.fit)
summary(lm.fit)
```

We now use the predict() function to estimate the response for all 392
observations, and we use the mean() function to calculate the MSE of the
196 observations in the validation set. Note that the -train index below
selects only the observations that are not in the training set.

```{r}
attach(Auto)
mean((mpg-predict(lm.fit, Auto))[-train]^2)
```







